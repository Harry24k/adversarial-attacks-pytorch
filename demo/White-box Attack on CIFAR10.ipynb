{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# White-box Attack on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:24\"\n",
    "\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "import torchattacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[Data loaded]\n",
      "[Model loaded]\n",
      "Acc: 94.78 %\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '..')\n",
    "import robustbench\n",
    "from robustbench.data import load_cifar10\n",
    "from robustbench.utils import load_model, clean_accuracy\n",
    "\n",
    "#images, labels = load_cifar10(n_examples=5)\n",
    "images, labels = load_cifar10()\n",
    "print('[Data loaded]')\n",
    "\n",
    "device = \"cuda\"\n",
    "model = load_model('Standard', norm='Linf').to(device)\n",
    "acc = clean_accuracy(model, images.to(device), labels.to(device))\n",
    "print('[Model loaded]')\n",
    "print('Acc: %2.2f %%'%(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from torchattacks import PGD, CosPGD\n",
    "from utils import imshow, get_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGD(model_name=WideResNet, device=cuda:0, attack_mode=default, targeted=False, normalization_used=False, eps=0.03137254901960784, alpha=0.008888888888888889, steps=10, random_start=True)\n"
     ]
    }
   ],
   "source": [
    "pgd_atk = PGD(model, eps=8/255, alpha=2/225, steps=10, random_start=True)\n",
    "print(pgd_atk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CosPGD(model_name=WideResNet, device=cuda:0, attack_mode=default, targeted=False, normalization_used=False, eps=0.03137254901960784, alpha=0.008888888888888889, steps=10, random_start=True)\n"
     ]
    }
   ],
   "source": [
    "cospgd_atk = CosPGD(model, eps=8/255, alpha=2/225, steps=10, random_start=True)\n",
    "print(cospgd_atk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# When normalization used:\n",
    "# atk.set_normalization_used(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    (images, labels),\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=128,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pgd_adv_images = pgd_atk(images[:100], labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.10 GiB (GPU 0; 23.64 GiB total capacity; 13.81 GiB already allocated; 1.30 GiB free; 13.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cospgd_adv_images \u001b[39m=\u001b[39m cospgd_atk(images, labels)\n",
      "File \u001b[0;32m~/Shashank_Projects/adversarial-attacks-pytorch/demo/../torchattacks/attack.py:458\u001b[0m, in \u001b[0;36mAttack.__call__\u001b[0;34m(self, images, labels, *args, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_change_model_mode(given_training)\n\u001b[1;32m    457\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_inputs(images)\n\u001b[0;32m--> 458\u001b[0m adv_images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(images, labels, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    459\u001b[0m adv_images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_outputs(adv_images)\n\u001b[1;32m    460\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recover_model_mode(given_training)\n",
      "File \u001b[0;32m~/Shashank_Projects/adversarial-attacks-pytorch/demo/../torchattacks/attacks/cospgd.py:67\u001b[0m, in \u001b[0;36mCosPGD.forward\u001b[0;34m(self, images, labels)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m     66\u001b[0m     adv_images\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_logits(adv_images)         \n\u001b[1;32m     68\u001b[0m     sig_output \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msigmoid(outputs) \n\u001b[1;32m     69\u001b[0m     \u001b[39m#import ipdb;ipdb.set_trace()           \u001b[39;00m\n",
      "File \u001b[0;32m~/Shashank_Projects/adversarial-attacks-pytorch/demo/../torchattacks/attack.py:89\u001b[0m, in \u001b[0;36mAttack.get_logits\u001b[0;34m(self, inputs, labels, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_normalization_applied:\n\u001b[1;32m     88\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize(inputs)\n\u001b[0;32m---> 89\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(inputs)\n\u001b[1;32m     90\u001b[0m \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/miniconda3/envs/adv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Shashank_Projects/adversarial-attacks-pytorch/demo/../robustbench/model_zoo/architectures/wide_resnet.py:88\u001b[0m, in \u001b[0;36mWideResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     87\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[0;32m---> 88\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock1(out)\n\u001b[1;32m     89\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock2(out)\n\u001b[1;32m     90\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock3(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/adv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Shashank_Projects/adversarial-attacks-pytorch/demo/../robustbench/model_zoo/architectures/wide_resnet.py:47\u001b[0m, in \u001b[0;36mNetworkBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/adv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/adv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/adv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Shashank_Projects/adversarial-attacks-pytorch/demo/../robustbench/model_zoo/architectures/wide_resnet.py:31\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdroprate \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     30\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(out, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdroprate, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m---> 31\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(out)\n\u001b[1;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39madd(x \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mequalInOut \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvShortcut(x), out)\n",
      "File \u001b[0;32m~/miniconda3/envs/adv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/adv/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/adv/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.10 GiB (GPU 0; 23.64 GiB total capacity; 13.81 GiB already allocated; 1.30 GiB free; 13.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "cospgd_adv_images = cospgd_atk(images[:100], labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 0\n",
    "pre = get_pred(model, pgd_adv_images[idx:idx+1], device)\n",
    "imshow(pgd_adv_images[idx:idx+1], title=\"True:%d, Pre:%d\"%(labels[idx], pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "pre = get_pred(model, cospgd_adv_images[idx:idx+1], device)\n",
    "imshow(cospgd_adv_images[idx:idx+1], title=\"True:%d, Pre:%d\"%(labels[idx], pre))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ceb8aea646a0c712ed5db194d127de24ece80f87032283552cbe7de982c3798"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
